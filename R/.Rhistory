source("C:/Users/pchan/OneDrive - University of Florida/grad school/PhD/research/RBF/R/rbf.R")
source("C:/Users/pchan/OneDrive - University of Florida/grad school/PhD/research/RBF/R/rbf.R")
source("C:/Users/pchan/OneDrive - University of Florida/grad school/PhD/research/RBF/R/rbf.R")
source("C:/Users/pchan/OneDrive - University of Florida/grad school/PhD/research/RBF/R/rbf.R")
1e-2
library(mvtnorm)
library(dplyr)
library(grf)
n = 180 #for three groups
x <-matrix(runif(5*n),nrow=n)
#Creating test and train dataset
train_proportion <- 2/3
train_size <- round(train_proportion*n)
train_indices <- sample(1:n, train_size)
x_train <- x[train_indices,]
#x_test <- x[train_indices,] #for the estimated values of the training set
x_test <- x[-train_indices,] #for the predicted values of the testing set
n <- train_proportion*n
f10 <- 10*sin(pi*x[,1]*x[,2])+20*(x[,3]-0.5)^2+(10*x[,4]+5*x[,5])
f20 <- (5*x[,2])/(1+x[,1]^2)+ 5*sin(x[,3]*x[,4]) + x[,5] #2*x[,3]+(x[,4]) + x[,5]
f30 <- 0.1*exp(4*x[,1]) + 4/(1 + exp(-20*(x[,2] - 0.5))) + 3*x[,3]^2 + 2*x[,4] + x[,5]
f1 <- f10+f20#/2
f2 <- (f10+f20)/2+f30/3
f3 <- (f20+f30)/2 + f10/3
#f4 <- (5*x[,2])/(1+x[,1]^2)+3*x[,3]+2*x[,4]+x[,5]
#treatment set up
z <- rep(seq(1:3), each = n/3)
gs <- length(unique(z))
#Generating outcomes from Normal(f_g, 1)
y1 <- rnorm(n/gs, mean = f1, sd = 1)
y2 <- rnorm(n/gs, mean = f2, sd = 1)
y3 <- rnorm(n/gs, mean = f3, sd = 1)
#Store all outcomes as a vector
y <- c(y1, y2, y3)
x <- x_train
####Number of Covariates####
p <- ncol(x)
####Apply min-max transformation on the response####
my <- min(y)+0.5*(max(y)-min(y))
sy  <- (max(y)-min(y))
y <- (y-my)/sy
####Finding optimal K####
gs <- length(unique(z))
Kls <- rep(14, gs)
index_list <- list()
for (g in 1:gs) {
#index list to group the response and predictors associated with the g-th treatment
index_list[[g]] <- which(z == g)
#utilize Gaussian RBF kernel-based relevance vector machine (RVM)
#used to obtain the group-specific number of relevance vectors
out <- try(foo <- kernlab::rvm(x[index_list[[g]],], y[index_list[[g]]],
kernel = "rbfdot"), silent = T)
if(class(out)!="try-error"){Kls[g] <- foo@nRV}
}
#set $K = G \times \max_g v_g$
K <- gs*max(Kls)+add
#Gaussian radial function for the RBF network for the model
X_mat_fun2 <- function(X, mu) {
X_mat <- matrix(0, nrow(X), K)
for (k in 1:K) {
X_mat[,k] <- exp(-(sqrt(rowSums(t(t(X) - mu[,k])^2))/sigma_k[k])^2)
}
return(X_mat)
}
#Gaussian radial function multiplied by the corresponding treatment group for the i-th patient, used for the posterior sampling of theta
X_star_fun2 <- function(X_mat, gamma) {
X_star_mat <- matrix(0, nrow(X_mat), K)
for (g in 1:gs) {
for (k in 1:K) {
X_star_mat[index_list[[g]],k] <- gamma[g, k] * X_mat[index_list[[g]], k]
}
}
return(X_star_mat)
}
#Similar function as above, but for a hypothetical different treatment group from the assigned treatment group, used to obtain CATE estimators
X_star_est <- function(X_mat, gamma, trt) {
X_star_e <- matrix(0, nrow(X_mat), K)
for (k in 1:K) {
X_star_e[,k] <- gamma[trt,k] * X_mat[, k]
}
return(X_star_e)
}
#Matrix multiplication of theta and gamma, used to simplify coding
V_func <- function(gamma, theta) {
V_mat <- matrix(0, gs, K)
for (g in 1:gs) {
for (k in 1:K) {
V_mat[g, k] <-gamma[g, k] * theta[k]
}
}
return(V_mat)
}
#Obtain errors, notice that from the model, \epsilon_i = y_i - f_g(x_i). We can use this for gradient sampling of \mu
y_xv_func <- function(y, X_mat, V, g) {
# y_xv <- matrix(0, gs)
#for (g in 1:2) {
y_xv <- sum((y[index_list[[g]]] - X_mat[index_list[[g]],] %*% V[g,])^2)
#}
return(y_xv)
}
#Conditional posterior log-likelihood function of model, used for gradient sampling of \mu
llhood_fun <- function(y, mu) {
X_mat <- X_mat_fun2(x, mu)
y_xv <-sum(sapply(1:gs, function(g) y_xv_func(y, X_mat, V, g)))
each <- -(1/(2*sigma^2))*y_xv #add prior information
return(each - sum(mu^2/(2*(sigma_mu)^2)))
}
#Derivative of the above log-likelihood function
derivmu2 <- function(y, x, V, mu, K, p, gs) {
X_mat <- X_mat_fun2(x, mu)
score_mat <- matrix(0, p, K)
groups <- matrix(0, gs, p)
for (k in 1:K) {
for (g in 1:gs) {
pr <- 2*(sqrt(rowSums(t(t(x[index_list[[g]],]) - mu[,k])^2))/sigma_k[k])
groups[g,] <- (1/sigma^2/2)*V[g,k]*colSums((t(t(x[index_list[[g]],]) - mu[,k])/sigma_k[k]^2) * c((y[index_list[[g]]] - X_mat[index_list[[g]],] %*% array(V[g,])) * (X_mat[index_list[[g]],k])*(pr*sigma_k[k]/sqrt(rowSums(t(t(x[index_list[[g]],]) - mu[,k])^2)))))
}
score_mat[,k] <- colSums(groups)
}
return(score_mat- mu/sigma_mu^2)
}
#initializing \mu with the entropy weighted k-means clustering algorithm
mu_ini <- as.matrix(t(wskm::ewkm(x, K)$centers))+matrix(rnorm(p*K, mean=0, sd=0.01), p,K)
p_j <- 0.5
#initializing \Gamma based on v_g, K
#creating G disjoints sets, and fixing small set of indices to be 1
gamma_ini <- matrix(0, gs, K)
for (g in 1:gs) {
gamma_ini[g, c(1:Kls[g]+sum(Kls[1:g-1]))] <- 1
}
#initializing other values using the functions earlier created
X_mat <- X_mat_fun2(x, mu_ini)
X_test_mat <- X_mat_fun2(x_test, mu_ini)
X_star_mat <- X_star_fun2(X_mat, gamma_ini)
add <- 10
add <- 0
#set $K = G \times \max_g v_g$
K <- gs*max(Kls)+add
#initializing \Gamma based on v_g, K
#creating G disjoints sets, and fixing small set of indices to be 1
gamma_ini <- matrix(0, gs, K)
for (g in 1:gs) {
gamma_ini[g, c(1:Kls[g]+sum(Kls[1:g-1]))] <- 1
}
#initializing other values using the functions earlier created
X_mat <- X_mat_fun2(x, mu_ini)
X_test_mat <- X_mat_fun2(x_test, mu_ini)
#initializing \mu with the entropy weighted k-means clustering algorithm
mu_ini <- as.matrix(t(wskm::ewkm(x, K)$centers))+matrix(rnorm(p*K, mean=0, sd=0.01), p,K)
sigma_mu <- 1#max(abs(x))
#pre-specified bandwidth parameters \sigma_k's, discussed in "Other computational settings"
if(is.null(sigma_k)){
for(k in 1:K){
mat <- as.matrix(dist(rbind(x, mu_ini[,k])))
sigma_k[k] <- mean(dist(t(mu_ini)))*sqrt(2)#*mean(mat[n+1,1:n])#mean(dist(t(mu_ini)))
}
}
p_j <- 0.5
#initializing \Gamma based on v_g, K
#creating G disjoints sets, and fixing small set of indices to be 1
gamma_ini <- matrix(0, gs, K)
sigma_k <- NULL
#pre-specified bandwidth parameters \sigma_k's, discussed in "Other computational settings"
if(is.null(sigma_k)){
for(k in 1:K){
mat <- as.matrix(dist(rbind(x, mu_ini[,k])))
sigma_k[k] <- mean(dist(t(mu_ini)))*sqrt(2)#*mean(mat[n+1,1:n])#mean(dist(t(mu_ini)))
}
}
p_j <- 0.5
#initializing \Gamma based on v_g, K
#creating G disjoints sets, and fixing small set of indices to be 1
gamma_ini <- matrix(0, gs, K)
for (g in 1:gs) {
gamma_ini[g, c(1:Kls[g]+sum(Kls[1:g-1]))] <- 1
}
#initializing other values using the functions earlier created
X_mat <- X_mat_fun2(x, mu_ini)
X_test_mat <- X_mat_fun2(x_test, mu_ini)
X_star_mat <- X_star_fun2(X_mat, gamma_ini)
cx <- crossprod(X_star_mat)
crossprod(X_star_mat)
X_star_mat * y
X_star_mat
X_star_mat * y
colSums(X_star_mat * y)
dim(X_star_mat)
dim(y)
y
X_star_mat
dim(X_star_mat)
length(y)
X_star_mat * y == X_star_mat %*% y
X_star_mat %*% y
X_star_mat * y
diag(cx)
mean(diag(cx))
nrow(cx)
diag(nrow(cx))
diag(nrow(cx))
nrow(cx)
diag(nrow(cx))
cx
x2 <- matrix(1:441, nrow = 21)
# Use diag(nrow(x)) to create a diagonal matrix
result <- diag(nrow(x2))
x2 <- matrix(1:441, nrow = 21)
# Use diag(nrow(x)) to create a diagonal matrix
result <- diag(nrow(x2))
result
mean(diag(cx))*diag(nrow(cx))
cx+1e-2*mean(diag(cx))*diag(nrow(cx))
solve(cx+1e-2*mean(diag(cx))*diag(nrow(cx)))
diag(nrow(cx)
)
diag(cx)
mean(diag(cx))*diag(nrow(cx))
solve(cx+1e-2*mean(diag(cx))*diag(nrow(cx))) %*% colSums(X_star_mat * y)
solve(cx+1e-2*mean(diag(cx))*diag(nrow(cx)))
mean(diag(cx))
diag(sum(diag(cx)))
sum(diag(cx))
sum(diag(cx))/100
1e-2*mean(diag(cx))*diag(nrow(cx)
)
solve(cx+1e-2*mean(diag(cx))*diag(nrow(cx))) %*% colSums(X_star_mat * y)
colSums(X_star_mat * y)
rep(1, K) %*% t(X_star_mat) %*% y
t(X_star_mat) %*% y
colSums(X_star_mat * y) == t(X_star_mat) %*% y
