for (g in 1:2) {
for (k in 1:K) {
X_star_mat[index_list[[g]],k] <- gamma[g, k] * X_mat[index_list[[g]], k]
}
}
return(X_star_mat)
}
#generating X_star_diff
X_star_diff <- function(X_mat, gamma) {
X_star_d <- matrix(0, n, K)
for (k in 1:K) {
X_star_d[,k] <- (gamma[2, k] - gamma[1,k]) * X_mat[, k]
}
return(X_star_d)
}
#X_star_diff(X_mat, gammas)
#generating V
V_func <- function(gamma, theta) {
V_mat <- matrix(0, 2, K)
for (g in 1:2) {
for (k in 1:K) {
V_mat[g, k] <-gamma[g, k] * theta[k]
}
}
return(V_mat)
}
#generating y-XV (for any situation, accounting for group)
y_xv_func <- function(y, X_mat, V, g) {
# y_xv <- matrix(0, gs)
#for (g in 1:2) {
y_xv <- (crossprod(y[index_list[[g]]] - X_mat[index_list[[g]],] %*% V[g,]))
#}
return(sum(y_xv))
}
#log-likelihood function
# llhood_fun <- function(y, V, mu, gs) {
#   X_mat <- X_mat_fun2(x, mu)
#   each <- rep(0, gs)
#   for (g in 1:2) {
#     each[g] <- -(1/(2*sigma^2))*(crossprod(y[index_list[[g]]] - X_mat[index_list[[g]],] %*% V[g,])) #add prior information
#   }
#   return(sum(each) - sum(mu^2/(2*(sigma_k)^2)))
# }
llhood_fun <- function(mu) {
X_mat <- X_mat_fun2(x, mu)
y_xv <- y_xv_func(y, X_mat, V, 1)+y_xv_func(y, X_mat, V, 2)
each <- -(1/(2*sigma^2))*y_xv #add prior information
return(each - sum(mu^2/(2*(sigma_mu)^2)))
}
#derivative of the log-likelihood function
derivmu2 <- function(y, x, V, mu, K, p, gs) {
X_mat <- X_mat_fun2(x, mu)
score_mat <- matrix(0, p, K)
groups <- matrix(0, gs, p)
for (k in 1:K) {
for (g in 1:gs) {
groups[g,] <- (2/sigma^2)*V[g,k]*colSums((t(t(x[index_list[[g]],]) - mu[,k])/sigma_k^2) * c((y[index_list[[g]]] - X_mat[index_list[[g]],] %*% array(V[g,])) * (X_mat[index_list[[g]],k])))
}
score_mat[,k] <- colSums(groups)
}
return(score_mat- mu/sigma_mu^2)
}
#proof that the derivative function is correct:
# derivmu_test <- function(y, x, V, mu, K, p, gs) {
#   X_mat <- X_mat_fun2(x, mu)
#   score_mat <- matrix(0, p, K)
#   groups <- matrix(0, gs, p)
#   for (k in 1:K) {
#     for (g in 1:gs) {
#       groups[g,] <- (2/sigma^2)*V[g,k]*colSums(t((y[index_list[[g]]] - X_mat[index_list[[g]],]%*%array(V[g,])) * (X_mat[index_list[[g]],k])) %*% t(t(x[index_list[[g]],]) - mu[,k])/sigma_k^2)
#       score_mat[,k] <- colSums(groups) #add prior
#     }
#   }
#   return(score_mat)
# }
#
# score_test <- derivmu_test(y, x, V, mu_ini, 10, 5, 2)
#
# llhood_fun2 <- function(mu) {
#   each <- rep(0, 2)
#   X_mat <- X_mat_fun2(x, mu)
#   for (g in 1:2) {
#     each[g] <- -(1/(2*sigma^2))*(crossprod(y[index_list[[g]]] - X_mat[index_list[[g]],] %*% V[g,])) #add prior information
#   }
#   return(sum(each))
# }
#
#
# all.equal(array(score_test), array(pracma::jacobian(llhood_fun2, mu_ini)))
####Initializing####
mu_ini <- matrix(rnorm(p*K, 0, 1), p, K)
sigma_mu <- 1
gamma_ini <- matrix(1, 2, K)
#gamma_ini[1, (K/2+1):K] <- 0
#gamma_ini[2, -c((K/2+1):K)] <- 0
X_mat <- X_mat_fun2(x, mu_ini)
X_star_mat <- X_star_fun2(X_mat, gamma_ini)
theta_ini <- solve(crossprod(X_mat)) %*% colSums(X_mat * y)
theta_0 <- rep(0, K)
V <- V_func(gamma_ini, theta_ini)
y_xv <- y_xv_func(y, X_mat, V, 1)+y_xv_func(y, X_mat, V, 2)
sigma_ini <- (1/(n-K-1))*y_xv
D_0inv <- diag(rep(1/10, K))
theta <- theta_ini
sigma <- sigma_ini
gammas <- gamma_ini
mu <- mu_ini
llhood <- llhood_fun(mu_ini)
itr <- 0
epsilon1 <- 1e-1
flag1 <- 0
####Sampling scheme#####
#Define lists to store postburn samples
thetals <- list()
gammals <- list()
sigmals <- list()
muls <- list()
treatls <- list()
Total_itr <- 10000
burn <- 5000 #Sample before itr==burn are in burn-in period.
while(itr < Total_itr){
start_time <- Sys.time()
itr <- itr + 1
#####Gibbs Sampling#####
###Sample theta conditional given gamma, sigma, y###
#theta_tilde <- solve(solve(D_0) + (1/sigma)*t(X_star_mat) %*% X_star_mat) %*% solve(D_0) %*% theta_0 + (1/sigma)*rowSums(X_star_mat *y)
theta_tilde <- solve(D_0inv + (1/sigma^2)*crossprod(X_star_mat)) %*% ((D_0inv %*% theta_0) + (1/sigma^2)*(t(X_star_mat)%*%y))
#D <- solve(solve(D_0) + (1/sigma)*x_star %*% x_star)
D <- solve(D_0inv + (1/sigma^2)*crossprod(X_star_mat))
#theta <- rnorm(10, theta_tilde, D)
theta <- array(rmvnorm(1,mean=theta_tilde,sigma=D))
###Sample gamma_j given gamma_-j, theta, sigma, y###
if (itr > 1000) {
for (g in 1:2) {
for (j in 1:K) {
gamma1 <- gamma0 <- gammas
gamma1[g, j] <- 1
gamma0[g, j] <- 0
#V_star <- gamma1*theta
V_star <- V_func(gamma1, theta)
#V_2star <- gamma0*theta
V_2star <- V_func(gamma0, theta)
y_xvstar <- y_xv_func(y, X_mat, V_star, g)
y_xv2star <- y_xv_func(y, X_mat, V_2star, g)
#c_j <- p_j*exp(-1/(2*sigma^2)*y_xvstar)
#d_j <- (1-p_j)*exp(-1/(2*sigma^2)*y_xv2star)
d_jDivc_j <- (1-p_j)*exp(-1/(2*sigma^2)*y_xv2star + 1/(2*sigma^2)*y_xvstar)/p_j
p_j_tilde <- 1/(1+d_jDivc_j)
gammas[g,j] <- rbinom(1, 1, p_j_tilde)
}
}
}
#update the X_star, V, y_xv with new thetas and gammas
V <- V_func(gammas, theta)
X_star_mat <- X_star_fun2(X_mat, gammas)
y_xv <- y_xv_func(y, X_mat, V, 1)+y_xv_func(y, X_mat, V, 2)
###Sample sigma given theta, gamma, y###
alpha <- 0.1
eta <- 0.1
sigma <- sqrt(1/rgamma(1, (alpha + n)/2, (eta + y_xv)/2))
####Gradient MH#####
temp <- mu + derivmu2(y, x, V, mu, K, p, 2) * epsilon1/2 + matrix(rnorm(1, sd = sqrt(epsilon1)), p, K)
muc <- temp
llhoodc <- llhood_fun(muc)
llhood <- llhood_fun(mu)
q1 <- -sum((mu-derivmu2(y, x, V, temp, K, p, 2)* epsilon1/2- temp)^2/epsilon1) /2
q2 <- -sum((temp-derivmu2(y, x, V, mu, K, p, 2) * epsilon1/2- mu)^2/epsilon1) /2
D_check <- llhoodc - llhood + q1 - q2
u <- runif(1)
if(log(u) < D_check){ #u < exp(D) [=ratio of likelihoods] P(log(u)<D)=P(u<exp(D))=exp(D)
mu <- muc
llhood <- llhoodc
X_mat <- X_mat_fun2(x, mu)
X_star_mat <- X_star_fun2(X_mat, gammas)
y_xv <- y_xv_func(y, X_mat, V, 1)+y_xv_func(y, X_mat, V, 2)
flag1 <- flag1 + 1 #To count number of accepts
}
####Update the treatment list####
trt_effect <- X_star_diff(X_mat, gammas) %*% theta
#gradient acceptance rate is usually around 0.45-0.7
if(itr %% 100==0){ #Auto-tuning step
ar <- flag1 / itr #Acceptance rate
if(ar < 0.45){ #Acceptance is too low
epsilon1 <- epsilon1 / 2
}
if(ar > 0.7){ #Acceptance is too high
epsilon1 <- epsilon1 * 2
}
print(ar) #To track the acceptance rate
print(mean((trt_effect - (f5-f1))^2) )
}
####Store the posterior samples####
if(itr > burn){
thetals[[itr - burn]] <- theta
gammals[[itr - burn]] <- gammas
sigmals[[itr - burn]] <- sigma
muls[[itr-burn]] <- mu
treatls[[itr-burn]] <- trt_effect
}
end_time <- Sys.time()
}
f5f1est <- Reduce('+', treatls)/length(treatls)
mean((f5f1est - (f5-f1))^2)
mean((sy*f5f1est - (f5-f1))^2)
####Data generation####
set.seed(123)
n = 100
x<-matrix(runif(500),nrow=100)
beta <- rnorm(5, 0, 5)
p <- exp(x %*% beta)/(1+exp(x %*% beta))
f1 <- 10*sin(pi*x[,1]*x[,2])+20*(x[,3]-0.5)^2+(10*x[,4]+5*x[,5])
f5 <- (5*x[,2])/(1+x[,1]^2)+3*x[,3]+2*x[,4]+x[,5]
z <- rbinom(100, 1, p)
y <- rnorm(100, (1-z)*f1 + z*f5, 1)
#scaling y
my <- mean(y)
sy <- sd(y)
y <- (y - my)/sy
#scaling x
x <- scale(x)
data <- as.data.frame(cbind(x, z, y))
colnames(data) <- c("x1", "x2", "x3", "x4", "x5", "z", "y")
####Finding optimal K####
foo <- kernlab::rvm(x, y,
kernel = "rbfdot",
kpar=list(sigma = 1))
kernlab::alpha(foo)
kernlab::RVindex(foo) #6 20 37 55 71
kernlab::kpar(foo)
####Setting####
set.seed(123)
n <- 100
K <- 10
p <- 5
p_j <- 0.5
sigma_k <- 3
#breaking X into two groups
X0_index <- which(data$z == 0)
X1_index <- which(data$z == 1)
index_list <- list()
index_list[[1]] <- X0_index
index_list[[2]] <- X1_index
X0 <- data[X0_index, -c(6,7)]
X1 <- data[X1_index, -c(6,7)]
####Functions#####
#generating X_mat
X_mat_fun2 <- function(X, mu) {
X_mat <- matrix(0, n, K)
for (k in 1:K) {
X_mat[,k] <- exp(-rowSums(t(t(X) - mu[,k])^2)/sigma_k^2)
}
return(X_mat)
}
#generating X_star_mat
X_star_fun2 <- function(X_mat, gamma) {
X_star_mat <- matrix(0, n, K)
for (g in 1:2) {
for (k in 1:K) {
X_star_mat[index_list[[g]],k] <- gamma[g, k] * X_mat[index_list[[g]], k]
}
}
return(X_star_mat)
}
#generating X_star_diff
X_star_diff <- function(X_mat, gamma) {
X_star_d <- matrix(0, n, K)
for (k in 1:K) {
X_star_d[,k] <- (gamma[2, k] - gamma[1,k]) * X_mat[, k]
}
return(X_star_d)
}
#X_star_diff(X_mat, gammas)
#generating V
V_func <- function(gamma, theta) {
V_mat <- matrix(0, 2, K)
for (g in 1:2) {
for (k in 1:K) {
V_mat[g, k] <-gamma[g, k] * theta[k]
}
}
return(V_mat)
}
#generating y-XV (for any situation, accounting for group)
y_xv_func <- function(y, X_mat, V, g) {
# y_xv <- matrix(0, gs)
#for (g in 1:2) {
y_xv <- (crossprod(y[index_list[[g]]] - X_mat[index_list[[g]],] %*% V[g,]))
#}
return(sum(y_xv))
}
#log-likelihood function
# llhood_fun <- function(y, V, mu, gs) {
#   X_mat <- X_mat_fun2(x, mu)
#   each <- rep(0, gs)
#   for (g in 1:2) {
#     each[g] <- -(1/(2*sigma^2))*(crossprod(y[index_list[[g]]] - X_mat[index_list[[g]],] %*% V[g,])) #add prior information
#   }
#   return(sum(each) - sum(mu^2/(2*(sigma_k)^2)))
# }
llhood_fun <- function(mu) {
X_mat <- X_mat_fun2(x, mu)
y_xv <- y_xv_func(y, X_mat, V, 1)+y_xv_func(y, X_mat, V, 2)
each <- -(1/(2*sigma^2))*y_xv #add prior information
return(each - sum(mu^2/(2*(sigma_mu)^2)))
}
#derivative of the log-likelihood function
derivmu2 <- function(y, x, V, mu, K, p, gs) {
X_mat <- X_mat_fun2(x, mu)
score_mat <- matrix(0, p, K)
groups <- matrix(0, gs, p)
for (k in 1:K) {
for (g in 1:gs) {
groups[g,] <- (2/sigma^2)*V[g,k]*colSums((t(t(x[index_list[[g]],]) - mu[,k])/sigma_k^2) * c((y[index_list[[g]]] - X_mat[index_list[[g]],] %*% array(V[g,])) * (X_mat[index_list[[g]],k])))
}
score_mat[,k] <- colSums(groups)
}
return(score_mat- mu/sigma_mu^2)
}
#proof that the derivative function is correct:
# derivmu_test <- function(y, x, V, mu, K, p, gs) {
#   X_mat <- X_mat_fun2(x, mu)
#   score_mat <- matrix(0, p, K)
#   groups <- matrix(0, gs, p)
#   for (k in 1:K) {
#     for (g in 1:gs) {
#       groups[g,] <- (2/sigma^2)*V[g,k]*colSums(t((y[index_list[[g]]] - X_mat[index_list[[g]],]%*%array(V[g,])) * (X_mat[index_list[[g]],k])) %*% t(t(x[index_list[[g]],]) - mu[,k])/sigma_k^2)
#       score_mat[,k] <- colSums(groups) #add prior
#     }
#   }
#   return(score_mat)
# }
#
# score_test <- derivmu_test(y, x, V, mu_ini, 10, 5, 2)
#
# llhood_fun2 <- function(mu) {
#   each <- rep(0, 2)
#   X_mat <- X_mat_fun2(x, mu)
#   for (g in 1:2) {
#     each[g] <- -(1/(2*sigma^2))*(crossprod(y[index_list[[g]]] - X_mat[index_list[[g]],] %*% V[g,])) #add prior information
#   }
#   return(sum(each))
# }
#
#
# all.equal(array(score_test), array(pracma::jacobian(llhood_fun2, mu_ini)))
####Initializing####
mu_ini <- matrix(rnorm(p*K, 0, 1), p, K)
sigma_mu <- 1
gamma_ini <- matrix(1, 2, K)
#gamma_ini[1, (K/2+1):K] <- 0
#gamma_ini[2, -c((K/2+1):K)] <- 0
X_mat <- X_mat_fun2(x, mu_ini)
X_star_mat <- X_star_fun2(X_mat, gamma_ini)
theta_ini <- solve(crossprod(X_mat)) %*% colSums(X_mat * y)
theta_0 <- rep(0, K)
V <- V_func(gamma_ini, theta_ini)
y_xv <- y_xv_func(y, X_mat, V, 1)+y_xv_func(y, X_mat, V, 2)
sigma_ini <- (1/(n-K-1))*y_xv
D_0inv <- diag(rep(1/10, K))
theta <- theta_ini
sigma <- sigma_ini
gammas <- gamma_ini
mu <- mu_ini
llhood <- llhood_fun(mu_ini)
itr <- 0
epsilon1 <- 1e-1
flag1 <- 0
####Sampling scheme#####
#Define lists to store postburn samples
thetals <- list()
gammals <- list()
sigmals <- list()
muls <- list()
treatls <- list()
Total_itr <- 10000
burn <- 5000 #Sample before itr==burn are in burn-in period.
while(itr < Total_itr){
start_time <- Sys.time()
itr <- itr + 1
#####Gibbs Sampling#####
###Sample theta conditional given gamma, sigma, y###
#theta_tilde <- solve(solve(D_0) + (1/sigma)*t(X_star_mat) %*% X_star_mat) %*% solve(D_0) %*% theta_0 + (1/sigma)*rowSums(X_star_mat *y)
theta_tilde <- solve(D_0inv + (1/sigma^2)*crossprod(X_star_mat)) %*% ((D_0inv %*% theta_0) + (1/sigma^2)*(t(X_star_mat)%*%y))
#D <- solve(solve(D_0) + (1/sigma)*x_star %*% x_star)
D <- solve(D_0inv + (1/sigma^2)*crossprod(X_star_mat))
#theta <- rnorm(10, theta_tilde, D)
theta <- array(rmvnorm(1,mean=theta_tilde,sigma=D))
###Sample gamma_j given gamma_-j, theta, sigma, y###
if (itr > 1000) {
for (g in 1:2) {
for (j in 1:K) {
gamma1 <- gamma0 <- gammas
gamma1[g, j] <- 1
gamma0[g, j] <- 0
#V_star <- gamma1*theta
V_star <- V_func(gamma1, theta)
#V_2star <- gamma0*theta
V_2star <- V_func(gamma0, theta)
y_xvstar <- y_xv_func(y, X_mat, V_star, g)
y_xv2star <- y_xv_func(y, X_mat, V_2star, g)
#c_j <- p_j*exp(-1/(2*sigma^2)*y_xvstar)
#d_j <- (1-p_j)*exp(-1/(2*sigma^2)*y_xv2star)
d_jDivc_j <- (1-p_j)*exp(-1/(2*sigma^2)*y_xv2star + 1/(2*sigma^2)*y_xvstar)/p_j
p_j_tilde <- 1/(1+d_jDivc_j)
gammas[g,j] <- rbinom(1, 1, p_j_tilde)
}
}
}
#update the X_star, V, y_xv with new thetas and gammas
V <- V_func(gammas, theta)
X_star_mat <- X_star_fun2(X_mat, gammas)
y_xv <- y_xv_func(y, X_mat, V, 1)+y_xv_func(y, X_mat, V, 2)
###Sample sigma given theta, gamma, y###
alpha <- 0.1
eta <- 0.1
sigma <- sqrt(1/rgamma(1, (alpha + n)/2, (eta + y_xv)/2))
####Gradient MH#####
temp <- mu + derivmu2(y, x, V, mu, K, p, 2) * epsilon1/2 + matrix(rnorm(1, sd = sqrt(epsilon1)), p, K)
muc <- temp
llhoodc <- llhood_fun(muc)
llhood <- llhood_fun(mu)
q1 <- -sum((mu-derivmu2(y, x, V, temp, K, p, 2)* epsilon1/2- temp)^2/epsilon1) /2
q2 <- -sum((temp-derivmu2(y, x, V, mu, K, p, 2) * epsilon1/2- mu)^2/epsilon1) /2
D_check <- llhoodc - llhood + q1 - q2
u <- runif(1)
if(log(u) < D_check){ #u < exp(D) [=ratio of likelihoods] P(log(u)<D)=P(u<exp(D))=exp(D)
mu <- muc
llhood <- llhoodc
X_mat <- X_mat_fun2(x, mu)
X_star_mat <- X_star_fun2(X_mat, gammas)
y_xv <- y_xv_func(y, X_mat, V, 1)+y_xv_func(y, X_mat, V, 2)
flag1 <- flag1 + 1 #To count number of accepts
}
####Update the treatment list####
trt_effect <- X_star_diff(X_mat, gammas) %*% theta
#gradient acceptance rate is usually around 0.45-0.7
if(itr %% 100==0){ #Auto-tuning step
ar <- flag1 / itr #Acceptance rate
if(ar < 0.45){ #Acceptance is too low
epsilon1 <- epsilon1 / 2
}
if(ar > 0.7){ #Acceptance is too high
epsilon1 <- epsilon1 * 2
}
print(ar) #To track the acceptance rate
print(mean((trt_effect - (f5-f1))^2) )
}
####Store the posterior samples####
if(itr > burn){
thetals[[itr - burn]] <- theta
gammals[[itr - burn]] <- gammas
sigmals[[itr - burn]] <- sigma
muls[[itr-burn]] <- mu
treatls[[itr-burn]] <- trt_effect
}
end_time <- Sys.time()
}
f5f1est <- Reduce('+', treatls)/length(treatls)
mean((f5f1est - (f5-f1))^2)
mean((sy*f5f1est - (f5-f1))^2)
sy
thetahat <- function(x) mean(x)
thetahat <- function(x) 1- 2*mean(x)
thetahat <- function(x) 1- 2*mean(x)
thetahat <- function(x) mean(x)
n <- 10
B <- 2000
x <- rnorm(n)
thetahat <- function(x) mean(x)
thetahat_0 <- thetahat(x)
xs <- matrix(sample(x, n*B, replace = T), B, n)
View(xs)
apply(xs, 1, thetahat)
thetahat_bs <- apply(xs, 1, thetahat)
hist(thetahat_bs)
density(thetahat_bs)
bias <- mean(thetahat_bs) - thetahat_0
bias
variance(thetahat_bs)
var(thetahat_bs)
?var
quantile(thetahat_bs, c(0.25, .75, 0.5))
?percentile
setwd("C:/Users/pchan/OneDrive - University of Florida/grad school/PhD/research/MultiTrtRBF")
setwd("C:/Users/pchan/OneDrive - University of Florida/grad school/PhD/research/MultiTrtRBF")
getwd(0)
getwd()
dataset <- readr::read_csv("C:/Users/pchan/OneDrive - University of Florida/grad school/PhD/research//test_data_locf.csv")[, -1]
dataset <- readr::read_csv("C:/Users/pchan/OneDrive - University of Florida/grad school/PhD/research/test_data_locf.csv")[, -1]
dataset <- readr::read_csv("C:/Users/pchan/OneDrive - University of Florida/grad school/PhD/research/database/test_data_locf.csv")[, -1]
usethis::use_data(dataset)
devtools::document()
devtools::document()
browseVignettes()
usethis::use_vignette("MultiTrtRBF")
devtools::install(build_vignettes = TRUE)
devtools::check()
devtools::build()
devtools::document()
devtools::test()
devtools::check()
source("../R/rbf.R")
devtools::check()
devtools::build()
devtools::install()
remove.packages("MultiTrtRBF")
devtools::document()
devtools::document()
devtools::install()
install.packages(file.path("C:/Users/pchan/OneDrive - University of Florida/grad school/PhD/research","MultiTrtRBF_0.0.0.9000.tar.gz"),repos=NULL,type="source")
install.packages(file.path("C:/Users/pchan/OneDrive - University of Florida/grad school/PhD/research","MultiTrtRBF_0.0.0.9000.tar.gz"),repos=NULL,type="source")
devtools::document()
